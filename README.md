

* * *
# 주요 Normalization 소개

</br>

## 정규화란?
- 데이터셋의 Numerical Value 범위 차이를 **왜곡하지 않고**, **공통 척도로 변경**하는 일련의 프로세스

## 정규화 / 표준화 방법론
- **Min-Max Normalization**
  - 데이터의 값의 **범위를 (0 ~ 1) 혹은 (-1 ~ 1)의 범위로 조정**하는 방법
  - **Outlier(이상치)에 영향을 많이 받는 문제**가 있음

- **Z-Score Standardization**
  - 데이터의 값을 **분포가 평균이 0, 편차가 1이 되도록 조정**하는 방법
  - **Outlier(이상치)에 영향을 상대적으로 적게 받음**, 값의 범위가 제한되지 않음

- **L1 Regularization (Lasso Regularization)**
  - 실제 값과 예측 값 사이의 오차들의 절댓값 합
  - 맨허튼 거리, 택시 거리로 많이 알려져 있으며, 택시가 도시의 블록 사이를 이동해 다른 지점으로 이동하는 것과 같음
  - 두 벡터 간의 최단 거리를 찾는데 사용되나, 블록 사이로 이동하기 때문에 경로는 다양하게 나타날 수 있음
  - 다양한 경로가 존재 할 수 있기 때문에, 중요한 가중치만 남길 수 있는 **Feature Selection이 가능**함
  - 비 중요 변수를 우선적으로 줄이기 때문에, 변수간 상관 관계가 높으면 성능이 하락함
  - L2 대비 Outlier에 더 Robust하여, **Sparse Model에 적합함**
  - 0에서 미분이 불가능하므로, **Gradient-Based Learning시 사용에 주의가 필요**함

- **L2 Regularization (Ridge Regularization, Euclidean Norm)**
  - 실제 값과 예측 값 오차들의 제곱 합
  - 두 벡터를 직선으로 연결한 최단 거리로 단 하나만 존재 할 수 있음
  - 다양한 경로가 존재 할 수 없기 때문에, Feature Selection이 불가능함
  - 크기가 큰 변수를 우선적으로 줄이기 때문에, **변수간 상관 관계가 높아도 성능이 우수**함
  - Weight Decay를 통해 가중치를 전반적으로 작게 만들어 학습 효과가 L1 대비 우수함

* * *

</br>


* * *

## 배치 정규화 (Batch Normalization)
- 학습 과정에서 데이터가 신경망을 타고 흐르면서 레이어와 레이어 사이의 데이터 분포가 달라지는 현상이 발생함
  특히, **Batch 단위로 다른 분포를 가지는 데이터를 일정한 평균과 분산을 가지도록 정규화**하여 안정화시키는 방법
  이는 심층 신경망에서 가중치의 미세한 변화가 중첩되어 레이어의 깊이가 깊어질수록 그 변화가 더욱 커지기 때문으로, **Internal Covariate Shift가 발생**하게됨
- 추론 과정에서는 Batch 단위로 평균과 분산을 구하기가 어려우므로, 학습 단계에서 사용된 적절한 평균과 분산을 저장하고, 추론 시에 이를 사용함
- 주요 장점
  - 학습 속도 향상
  - **초기화 민감도 감소**
  - **일반화 성능 향상**
- 주요 단점
  - **학습시 Batch Size에 매우 민감함**
    - Batch Size가 너무 작으면 통계치가 불안정해질 수 있고, 너무 크면 메모리 사용량이 증가함
  - **RNN(LSTM)에 적용하기 어려워** 이 경우에는 Layer Normalization, Instance Normalization 등이 더 적합함
  - 학습/추론시 연산 비용이 증가
    - 특히, 분산 학습(Distributed Learning)에서 각 노드 간의 동기화가 필요하여, 분산 시스템에서는 추가적인 복잡성을 유발 할 수 있음
    - 모델의 구조가 복잡해지며, 이는 모델의 파라미터 수를 증가시키고, 최적화 과정에 추가적인 부담을 줄 수 있음

## Instance Normalization
- 추가 설명 필요
  
## Layer Normalization
- 추가 설명 필요

## Group Normalization
- 추가 설명 필요




